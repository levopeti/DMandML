{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28390f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 08:16:02.685026: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64\n",
      "2021-12-03 08:16:02.685058: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4faa61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train, X_test, inverse_target_map = get_data(min_size=None, min_size_test=None, fill_nan=None)\n",
    "train_columns = list(XY_train.columns)\n",
    "train_columns.remove(\"TARGET_NUM\")\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(2, 10))\n",
    "X_train_minmax = min_max_scaler.fit(XY_train[train_columns])\n",
    "x_train = X_train_minmax.transform(XY_train[train_columns])\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=1)\n",
    "imp_train = imp.fit(x_train)\n",
    "x_train_full = imp_train.transform(x_train)\n",
    "\n",
    "x_train_full_df = pd.DataFrame(x_train_full, columns=train_columns, index=XY_train.index)\n",
    "\n",
    "min_size = 150\n",
    "\n",
    "for c in x_train_full_df.columns:\n",
    "    if c != \"TARGET_NUM\":\n",
    "        x_train_full_df[c][x_train_full_df.groupby(c)[c].transform('size') <= min_size] = 0\n",
    "\n",
    "stand_scaler = StandardScaler()\n",
    "X_train_stand = stand_scaler.fit(x_train_full_df[train_columns])\n",
    "x_train_stand = X_train_stand.transform(x_train_full_df[train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939690b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cn, max_value in max_v.items():\n",
    "#    if cn != \"TARGET_NUM\":\n",
    "#        XY_train[cn] = XY_train[cn] / max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2dcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = XY_train[train_columns].values\n",
    "#y_train = to_categorical(XY_train[\"TARGET_NUM\"].values)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ad6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(XY_train[\"TARGET_NUM\"].values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_stand, y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8d621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2ef01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a28bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(inpit_shape):\n",
    "    ip = Input(shape=(inpit_shape,), name=\"input\")\n",
    "    #x = Dense(units=1024, name=\"hiddel_layer\", activation=\"relu\")(x)\n",
    "    x = Dense(units=512, name=\"hiddel_layer_2\", activation=\"relu\")(ip)\n",
    "    x = Dense(units=64, name=\"hiddel_layer_3\", activation=\"relu\")(x)\n",
    "    op = Dense(units=19, name=\"prediction\", activation=\"softmax\")(x)\n",
    "    model = Model(inputs=ip, outputs=op, name=\"full_model\")\n",
    "    model.summary()\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    optimizer = Adam(learning_rate, amsgrad=True)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa07ead9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 43)]              0         \n",
      "                                                                 \n",
      " hiddel_layer (Dense)        (None, 1024)              45056     \n",
      "                                                                 \n",
      " hiddel_layer_2 (Dense)      (None, 512)               524800    \n",
      "                                                                 \n",
      " hiddel_layer_3 (Dense)      (None, 64)                32832     \n",
      "                                                                 \n",
      " prediction (Dense)          (None, 19)                1235      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 603,923\n",
      "Trainable params: 603,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.2536 - accuracy: 0.2790 - val_loss: 2.2235 - val_accuracy: 0.2868\n",
      "Epoch 2/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.2164 - accuracy: 0.2889 - val_loss: 2.2096 - val_accuracy: 0.2911\n",
      "Epoch 3/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.2053 - accuracy: 0.2917 - val_loss: 2.2034 - val_accuracy: 0.2923\n",
      "Epoch 4/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1983 - accuracy: 0.2936 - val_loss: 2.2007 - val_accuracy: 0.2930\n",
      "Epoch 5/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1933 - accuracy: 0.2952 - val_loss: 2.1979 - val_accuracy: 0.2943\n",
      "Epoch 6/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1893 - accuracy: 0.2959 - val_loss: 2.1989 - val_accuracy: 0.2938\n",
      "Epoch 7/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1860 - accuracy: 0.2969 - val_loss: 2.2001 - val_accuracy: 0.2917\n",
      "Epoch 8/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1830 - accuracy: 0.2976 - val_loss: 2.1943 - val_accuracy: 0.2953\n",
      "Epoch 9/150\n",
      "24765/24765 [==============================] - 117s 5ms/step - loss: 2.1802 - accuracy: 0.2983 - val_loss: 2.1931 - val_accuracy: 0.2959\n",
      "Epoch 10/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.1775 - accuracy: 0.2989 - val_loss: 2.1953 - val_accuracy: 0.2958\n",
      "Epoch 11/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.1753 - accuracy: 0.2995 - val_loss: 2.1971 - val_accuracy: 0.2936\n",
      "Epoch 12/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1729 - accuracy: 0.3002 - val_loss: 2.1932 - val_accuracy: 0.2950\n",
      "Epoch 13/150\n",
      "24765/24765 [==============================] - 120s 5ms/step - loss: 2.1706 - accuracy: 0.3009 - val_loss: 2.1941 - val_accuracy: 0.2953\n",
      "Epoch 14/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.1684 - accuracy: 0.3014 - val_loss: 2.1953 - val_accuracy: 0.2956\n",
      "Epoch 15/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1661 - accuracy: 0.3021 - val_loss: 2.1953 - val_accuracy: 0.2955\n",
      "Epoch 16/150\n",
      "24765/24765 [==============================] - 118s 5ms/step - loss: 2.1638 - accuracy: 0.3029 - val_loss: 2.1942 - val_accuracy: 0.2958\n",
      "Epoch 17/150\n",
      "24765/24765 [==============================] - 117s 5ms/step - loss: 2.1619 - accuracy: 0.3030 - val_loss: 2.1962 - val_accuracy: 0.2954\n",
      "Epoch 18/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1597 - accuracy: 0.3038 - val_loss: 2.1962 - val_accuracy: 0.2954\n",
      "Epoch 19/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1573 - accuracy: 0.3044 - val_loss: 2.1971 - val_accuracy: 0.2948\n",
      "Epoch 20/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1555 - accuracy: 0.3049 - val_loss: 2.1984 - val_accuracy: 0.2948\n",
      "Epoch 21/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1532 - accuracy: 0.3055 - val_loss: 2.1983 - val_accuracy: 0.2959\n",
      "Epoch 22/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1510 - accuracy: 0.3060 - val_loss: 2.2005 - val_accuracy: 0.2948\n",
      "Epoch 23/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1490 - accuracy: 0.3067 - val_loss: 2.1994 - val_accuracy: 0.2951\n",
      "Epoch 24/150\n",
      "24765/24765 [==============================] - 117s 5ms/step - loss: 2.1469 - accuracy: 0.3073 - val_loss: 2.2029 - val_accuracy: 0.2945\n",
      "Epoch 25/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.1445 - accuracy: 0.3078 - val_loss: 2.2021 - val_accuracy: 0.2945\n",
      "Epoch 26/150\n",
      "24765/24765 [==============================] - 119s 5ms/step - loss: 2.1426 - accuracy: 0.3085 - val_loss: 2.2020 - val_accuracy: 0.2950\n",
      "Epoch 27/150\n",
      "24765/24765 [==============================] - 118s 5ms/step - loss: 2.1403 - accuracy: 0.3091 - val_loss: 2.2041 - val_accuracy: 0.2936\n",
      "Epoch 28/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1383 - accuracy: 0.3094 - val_loss: 2.2060 - val_accuracy: 0.2934\n",
      "Epoch 29/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.1361 - accuracy: 0.3099 - val_loss: 2.2083 - val_accuracy: 0.2939\n",
      "Epoch 30/150\n",
      "24765/24765 [==============================] - 111s 5ms/step - loss: 2.1340 - accuracy: 0.3108 - val_loss: 2.2092 - val_accuracy: 0.2929\n",
      "Epoch 31/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.1319 - accuracy: 0.3113 - val_loss: 2.2087 - val_accuracy: 0.2932\n",
      "Epoch 32/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.1299 - accuracy: 0.3118 - val_loss: 2.2103 - val_accuracy: 0.2940\n",
      "Epoch 33/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.1278 - accuracy: 0.3124 - val_loss: 2.2119 - val_accuracy: 0.2928\n",
      "Epoch 34/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.1258 - accuracy: 0.3130 - val_loss: 2.2126 - val_accuracy: 0.2926\n",
      "Epoch 35/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1237 - accuracy: 0.3135 - val_loss: 2.2155 - val_accuracy: 0.2921\n",
      "Epoch 36/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.1218 - accuracy: 0.3141 - val_loss: 2.2130 - val_accuracy: 0.2935\n",
      "Epoch 37/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1198 - accuracy: 0.3143 - val_loss: 2.2137 - val_accuracy: 0.2935\n",
      "Epoch 38/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1178 - accuracy: 0.3151 - val_loss: 2.2198 - val_accuracy: 0.2917\n",
      "Epoch 39/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.1159 - accuracy: 0.3156 - val_loss: 2.2223 - val_accuracy: 0.2902\n",
      "Epoch 40/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.1137 - accuracy: 0.3160 - val_loss: 2.2198 - val_accuracy: 0.2924\n",
      "Epoch 41/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.1119 - accuracy: 0.3168 - val_loss: 2.2233 - val_accuracy: 0.2920\n",
      "Epoch 42/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1100 - accuracy: 0.3170 - val_loss: 2.2247 - val_accuracy: 0.2921\n",
      "Epoch 43/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1081 - accuracy: 0.3179 - val_loss: 2.2255 - val_accuracy: 0.2907\n",
      "Epoch 44/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.1062 - accuracy: 0.3183 - val_loss: 2.2287 - val_accuracy: 0.2929\n",
      "Epoch 45/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.1044 - accuracy: 0.3186 - val_loss: 2.2241 - val_accuracy: 0.2921\n",
      "Epoch 46/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.1025 - accuracy: 0.3195 - val_loss: 2.2313 - val_accuracy: 0.2920\n",
      "Epoch 47/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.1007 - accuracy: 0.3194 - val_loss: 2.2331 - val_accuracy: 0.2899\n",
      "Epoch 48/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0990 - accuracy: 0.3201 - val_loss: 2.2324 - val_accuracy: 0.2917\n",
      "Epoch 49/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0972 - accuracy: 0.3206 - val_loss: 2.2339 - val_accuracy: 0.2916\n",
      "Epoch 50/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0955 - accuracy: 0.3213 - val_loss: 2.2361 - val_accuracy: 0.2915\n",
      "Epoch 51/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0938 - accuracy: 0.3216 - val_loss: 2.2359 - val_accuracy: 0.2903\n",
      "Epoch 52/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0920 - accuracy: 0.3224 - val_loss: 2.2392 - val_accuracy: 0.2904\n",
      "Epoch 53/150\n",
      "24765/24765 [==============================] - 111s 5ms/step - loss: 2.0903 - accuracy: 0.3225 - val_loss: 2.2387 - val_accuracy: 0.2886\n",
      "Epoch 54/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0885 - accuracy: 0.3232 - val_loss: 2.2439 - val_accuracy: 0.2901\n",
      "Epoch 55/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0869 - accuracy: 0.3235 - val_loss: 2.2450 - val_accuracy: 0.2911\n",
      "Epoch 56/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.0852 - accuracy: 0.3239 - val_loss: 2.2475 - val_accuracy: 0.2874\n",
      "Epoch 57/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0836 - accuracy: 0.3246 - val_loss: 2.2472 - val_accuracy: 0.2900\n",
      "Epoch 58/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0817 - accuracy: 0.3249 - val_loss: 2.2455 - val_accuracy: 0.2903\n",
      "Epoch 59/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0804 - accuracy: 0.3252 - val_loss: 2.2481 - val_accuracy: 0.2895\n",
      "Epoch 60/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0788 - accuracy: 0.3256 - val_loss: 2.2487 - val_accuracy: 0.2897\n",
      "Epoch 61/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0773 - accuracy: 0.3259 - val_loss: 2.2509 - val_accuracy: 0.2895\n",
      "Epoch 62/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0758 - accuracy: 0.3264 - val_loss: 2.2531 - val_accuracy: 0.2879\n",
      "Epoch 63/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0742 - accuracy: 0.3270 - val_loss: 2.2575 - val_accuracy: 0.2886\n",
      "Epoch 64/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0729 - accuracy: 0.3273 - val_loss: 2.2562 - val_accuracy: 0.2883\n",
      "Epoch 65/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0713 - accuracy: 0.3279 - val_loss: 2.2587 - val_accuracy: 0.2889\n",
      "Epoch 66/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0699 - accuracy: 0.3284 - val_loss: 2.2589 - val_accuracy: 0.2892\n",
      "Epoch 67/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0684 - accuracy: 0.3285 - val_loss: 2.2568 - val_accuracy: 0.2897\n",
      "Epoch 68/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0668 - accuracy: 0.3289 - val_loss: 2.2601 - val_accuracy: 0.2880\n",
      "Epoch 69/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0653 - accuracy: 0.3297 - val_loss: 2.2625 - val_accuracy: 0.2882\n",
      "Epoch 70/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0639 - accuracy: 0.3300 - val_loss: 2.2609 - val_accuracy: 0.2892\n",
      "Epoch 71/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0627 - accuracy: 0.3299 - val_loss: 2.2686 - val_accuracy: 0.2878\n",
      "Epoch 72/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0612 - accuracy: 0.3306 - val_loss: 2.2650 - val_accuracy: 0.2884\n",
      "Epoch 73/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0599 - accuracy: 0.3306 - val_loss: 2.2722 - val_accuracy: 0.2858\n",
      "Epoch 74/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0584 - accuracy: 0.3313 - val_loss: 2.2668 - val_accuracy: 0.2877\n",
      "Epoch 75/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0572 - accuracy: 0.3318 - val_loss: 2.2714 - val_accuracy: 0.2871\n",
      "Epoch 76/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0561 - accuracy: 0.3321 - val_loss: 2.2712 - val_accuracy: 0.2884\n",
      "Epoch 77/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0548 - accuracy: 0.3326 - val_loss: 2.2726 - val_accuracy: 0.2880\n",
      "Epoch 78/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0532 - accuracy: 0.3324 - val_loss: 2.2776 - val_accuracy: 0.2867\n",
      "Epoch 79/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0520 - accuracy: 0.3330 - val_loss: 2.2765 - val_accuracy: 0.2859\n",
      "Epoch 80/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0509 - accuracy: 0.3334 - val_loss: 2.2810 - val_accuracy: 0.2868\n",
      "Epoch 81/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0496 - accuracy: 0.3335 - val_loss: 2.2784 - val_accuracy: 0.2879\n",
      "Epoch 82/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0485 - accuracy: 0.3337 - val_loss: 2.2774 - val_accuracy: 0.2870\n",
      "Epoch 83/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0472 - accuracy: 0.3346 - val_loss: 2.2851 - val_accuracy: 0.2853\n",
      "Epoch 84/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0463 - accuracy: 0.3345 - val_loss: 2.2871 - val_accuracy: 0.2863\n",
      "Epoch 85/150\n",
      "24765/24765 [==============================] - 110s 4ms/step - loss: 2.0450 - accuracy: 0.3348 - val_loss: 2.2888 - val_accuracy: 0.2872\n",
      "Epoch 86/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0436 - accuracy: 0.3355 - val_loss: 2.2896 - val_accuracy: 0.2869\n",
      "Epoch 87/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0426 - accuracy: 0.3357 - val_loss: 2.2890 - val_accuracy: 0.2857\n",
      "Epoch 88/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0415 - accuracy: 0.3359 - val_loss: 2.2904 - val_accuracy: 0.2869\n",
      "Epoch 89/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.0401 - accuracy: 0.3364 - val_loss: 2.2896 - val_accuracy: 0.2868\n",
      "Epoch 90/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0390 - accuracy: 0.3365 - val_loss: 2.2936 - val_accuracy: 0.2859\n",
      "Epoch 91/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0378 - accuracy: 0.3371 - val_loss: 2.2957 - val_accuracy: 0.2851\n",
      "Epoch 92/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0367 - accuracy: 0.3372 - val_loss: 2.2920 - val_accuracy: 0.2863\n",
      "Epoch 93/150\n",
      "24765/24765 [==============================] - 117s 5ms/step - loss: 2.0358 - accuracy: 0.3375 - val_loss: 2.2969 - val_accuracy: 0.2850\n",
      "Epoch 94/150\n",
      "24765/24765 [==============================] - 117s 5ms/step - loss: 2.0349 - accuracy: 0.3377 - val_loss: 2.2964 - val_accuracy: 0.2839\n",
      "Epoch 95/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0335 - accuracy: 0.3382 - val_loss: 2.2987 - val_accuracy: 0.2859\n",
      "Epoch 96/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0326 - accuracy: 0.3385 - val_loss: 2.3005 - val_accuracy: 0.2839\n",
      "Epoch 97/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0312 - accuracy: 0.3388 - val_loss: 2.3008 - val_accuracy: 0.2843\n",
      "Epoch 98/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0304 - accuracy: 0.3392 - val_loss: 2.3095 - val_accuracy: 0.2858\n",
      "Epoch 99/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.0296 - accuracy: 0.3392 - val_loss: 2.3065 - val_accuracy: 0.2845\n",
      "Epoch 100/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0283 - accuracy: 0.3395 - val_loss: 2.3095 - val_accuracy: 0.2845\n",
      "Epoch 101/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0274 - accuracy: 0.3397 - val_loss: 2.3064 - val_accuracy: 0.2862\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0267 - accuracy: 0.3399 - val_loss: 2.3113 - val_accuracy: 0.2843\n",
      "Epoch 103/150\n",
      "24765/24765 [==============================] - 116s 5ms/step - loss: 2.0255 - accuracy: 0.3403 - val_loss: 2.3183 - val_accuracy: 0.2836\n",
      "Epoch 104/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0248 - accuracy: 0.3407 - val_loss: 2.3106 - val_accuracy: 0.2847\n",
      "Epoch 105/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0237 - accuracy: 0.3407 - val_loss: 2.3108 - val_accuracy: 0.2838\n",
      "Epoch 106/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0228 - accuracy: 0.3410 - val_loss: 2.3141 - val_accuracy: 0.2843\n",
      "Epoch 107/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0219 - accuracy: 0.3414 - val_loss: 2.3180 - val_accuracy: 0.2839\n",
      "Epoch 108/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0212 - accuracy: 0.3416 - val_loss: 2.3169 - val_accuracy: 0.2841\n",
      "Epoch 109/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0202 - accuracy: 0.3416 - val_loss: 2.3244 - val_accuracy: 0.2840\n",
      "Epoch 110/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0194 - accuracy: 0.3421 - val_loss: 2.3175 - val_accuracy: 0.2847\n",
      "Epoch 111/150\n",
      "24765/24765 [==============================] - 115s 5ms/step - loss: 2.0187 - accuracy: 0.3420 - val_loss: 2.3158 - val_accuracy: 0.2844\n",
      "Epoch 112/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0175 - accuracy: 0.3425 - val_loss: 2.3195 - val_accuracy: 0.2845\n",
      "Epoch 113/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0168 - accuracy: 0.3429 - val_loss: 2.3211 - val_accuracy: 0.2826\n",
      "Epoch 114/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0159 - accuracy: 0.3428 - val_loss: 2.3247 - val_accuracy: 0.2830\n",
      "Epoch 115/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0153 - accuracy: 0.3430 - val_loss: 2.3266 - val_accuracy: 0.2846\n",
      "Epoch 116/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0144 - accuracy: 0.3434 - val_loss: 2.3217 - val_accuracy: 0.2834\n",
      "Epoch 117/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 2.0137 - accuracy: 0.3437 - val_loss: 2.3300 - val_accuracy: 0.2821\n",
      "Epoch 118/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0126 - accuracy: 0.3439 - val_loss: 2.3300 - val_accuracy: 0.2842\n",
      "Epoch 119/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0119 - accuracy: 0.3441 - val_loss: 2.3322 - val_accuracy: 0.2830\n",
      "Epoch 120/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0110 - accuracy: 0.3445 - val_loss: 2.3380 - val_accuracy: 0.2820\n",
      "Epoch 121/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0102 - accuracy: 0.3446 - val_loss: 2.3365 - val_accuracy: 0.2817\n",
      "Epoch 122/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0094 - accuracy: 0.3448 - val_loss: 2.3387 - val_accuracy: 0.2812\n",
      "Epoch 123/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0087 - accuracy: 0.3450 - val_loss: 2.3350 - val_accuracy: 0.2847\n",
      "Epoch 124/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0081 - accuracy: 0.3453 - val_loss: 2.3448 - val_accuracy: 0.2830\n",
      "Epoch 125/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0075 - accuracy: 0.3452 - val_loss: 2.3352 - val_accuracy: 0.2839\n",
      "Epoch 126/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0067 - accuracy: 0.3455 - val_loss: 2.3420 - val_accuracy: 0.2829\n",
      "Epoch 127/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0058 - accuracy: 0.3458 - val_loss: 2.3385 - val_accuracy: 0.2819\n",
      "Epoch 128/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0051 - accuracy: 0.3459 - val_loss: 2.3393 - val_accuracy: 0.2825\n",
      "Epoch 129/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0045 - accuracy: 0.3460 - val_loss: 2.3418 - val_accuracy: 0.2812\n",
      "Epoch 130/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0037 - accuracy: 0.3464 - val_loss: 2.3458 - val_accuracy: 0.2825\n",
      "Epoch 131/150\n",
      "24765/24765 [==============================] - 113s 5ms/step - loss: 2.0030 - accuracy: 0.3463 - val_loss: 2.3416 - val_accuracy: 0.2823\n",
      "Epoch 132/150\n",
      "24765/24765 [==============================] - 112s 5ms/step - loss: 2.0023 - accuracy: 0.3467 - val_loss: 2.3482 - val_accuracy: 0.2828\n",
      "Epoch 133/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0017 - accuracy: 0.3470 - val_loss: 2.3457 - val_accuracy: 0.2827\n",
      "Epoch 134/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0009 - accuracy: 0.3471 - val_loss: 2.3437 - val_accuracy: 0.2815\n",
      "Epoch 135/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 2.0002 - accuracy: 0.3473 - val_loss: 2.3507 - val_accuracy: 0.2804\n",
      "Epoch 136/150\n",
      "24765/24765 [==============================] - 111s 4ms/step - loss: 1.9994 - accuracy: 0.3474 - val_loss: 2.3467 - val_accuracy: 0.2833\n",
      "Epoch 137/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 1.9989 - accuracy: 0.3479 - val_loss: 2.3490 - val_accuracy: 0.2836\n",
      "Epoch 138/150\n",
      "24765/24765 [==============================] - 114s 5ms/step - loss: 1.9983 - accuracy: 0.3479 - val_loss: 2.3529 - val_accuracy: 0.2800\n",
      "Epoch 139/150\n",
      "10439/24765 [===========>..................] - ETA: 55s - loss: 1.9924 - accuracy: 0.3497"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15131/12108701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(X_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aidrms/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = define_model(X_train.shape[1])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=150,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa9bde",
   "metadata": {},
   "source": [
    "## PCA + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583332f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 13:59:56.871595: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64\n",
      "2021-12-03 13:59:56.871628: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f4f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train, X_test, inverse_target_map = get_data(min_size=None, min_size_test=None, fill_nan=None)\n",
    "train_columns = list(XY_train.columns)\n",
    "train_columns.remove(\"TARGET_NUM\")\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(2, 10))\n",
    "X_train_minmax = min_max_scaler.fit(XY_train[train_columns])\n",
    "x_train = X_train_minmax.transform(XY_train[train_columns])\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=1)\n",
    "imp_train = imp.fit(x_train)\n",
    "x_train_full = imp_train.transform(x_train)\n",
    "\n",
    "x_train_full_df = pd.DataFrame(x_train_full, columns=train_columns, index=XY_train.index)\n",
    "\n",
    "min_size = 150\n",
    "\n",
    "for c in x_train_full_df.columns:\n",
    "    if c != \"TARGET_NUM\":\n",
    "        x_train_full_df[c][x_train_full_df.groupby(c)[c].transform('size') <= min_size] = 0\n",
    "\n",
    "stand_scaler = StandardScaler()\n",
    "X_train_stand = stand_scaler.fit(x_train_full_df[train_columns])\n",
    "x_train_stand = X_train_stand.transform(x_train_full_df[train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ad63962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "pca_result = pca.fit_transform(x_train_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "856e7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(XY_train[\"TARGET_NUM\"].values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_result, y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e6786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 30)]              0         \n",
      "                                                                 \n",
      " hiddel_layer_2 (Dense)      (None, 512)               15872     \n",
      "                                                                 \n",
      " hiddel_layer_3 (Dense)      (None, 64)                32832     \n",
      "                                                                 \n",
      " prediction (Dense)          (None, 19)                1235      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,939\n",
      "Trainable params: 49,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "24765/24765 [==============================] - 61s 2ms/step - loss: 2.2573 - accuracy: 0.2776 - val_loss: 2.2320 - val_accuracy: 0.2836\n",
      "Epoch 2/150\n",
      "22644/24765 [==========================>...] - ETA: 4s - loss: 2.2242 - accuracy: 0.2859"
     ]
    }
   ],
   "source": [
    "model = define_model(X_train.shape[1])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=150,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa225b",
   "metadata": {},
   "source": [
    "## Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540fea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Condition_importance', 'Hospital_death_flag', 'Age', 'Alanine Aminotransferase (ALT)', 'Alkaline Phosphatase', 'Anion Gap', 'Asparate Aminotransferase (AST)', 'Base Excess', 'Bicarbonate', 'Bilirubin, Total', 'Calcium, Total', 'Calculated Total CO2', 'Chloride', 'Creatinine', 'Glucose', 'H', 'Hematocrit', 'Hemoglobin', 'I', 'INR(PT)', 'L', 'MCH', 'MCHC', 'MCV', 'Magnesium', 'PT', 'PTT', 'Phosphate', 'Platelet Count', 'Potassium', 'RDW', 'RDW-SD', 'Red Blood Cells', 'Sodium', 'Urea Nitrogen', 'White Blood Cells', 'pCO2', 'pH', 'pO2', 'Admission_type_NUM', 'Marital_status_NUM', 'Ethnicity_NUM', 'Gender_NUM', 'TARGET_NUM']\n"
     ]
    }
   ],
   "source": [
    "print(list(XY_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c4182f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_drop = [\"Alanine Aminotransferase (ALT)\", \"Alkaline Phosphatase\", \"Base Excess\", \"Bicarbonate\",\n",
    "                    \"Glucose\", \"H\", \"INR(PT)\", \"MCHC\", \"PTT\", \"Platelet Count\", \"Potassium\", \"RDW\", \"RDW-SD\",\n",
    "                    \"Red Blood Cells\", \"pCO2\", \"pH\", \"pO2\", \"Admission_type_NUM\", \"Marital_status_NUM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5397df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_filt = XY_train.drop(columns_for_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb6ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition_importance</th>\n",
       "      <th>Hospital_death_flag</th>\n",
       "      <th>Age</th>\n",
       "      <th>Anion Gap</th>\n",
       "      <th>Asparate Aminotransferase (AST)</th>\n",
       "      <th>Bilirubin, Total</th>\n",
       "      <th>Calcium, Total</th>\n",
       "      <th>Calculated Total CO2</th>\n",
       "      <th>Chloride</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>...</th>\n",
       "      <th>MCV</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>PT</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Urea Nitrogen</th>\n",
       "      <th>White Blood Cells</th>\n",
       "      <th>Ethnicity_NUM</th>\n",
       "      <th>Gender_NUM</th>\n",
       "      <th>TARGET_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>12.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>140.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>136.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>138.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>134.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696240</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696241</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>131.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696242</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>128.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696243</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696244</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>31.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3696245 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Condition_importance  Hospital_death_flag   Age  Anion Gap  \\\n",
       "0                         4.0                  0.0  56.0       13.0   \n",
       "1                         7.0                  0.0  46.0       14.0   \n",
       "2                         1.0                  0.0  69.0       18.0   \n",
       "3                        10.0                  0.0  76.0       12.0   \n",
       "4                         8.0                  0.0  91.0       -2.0   \n",
       "...                       ...                  ...   ...        ...   \n",
       "3696240                   3.0                  0.0   0.0       -2.0   \n",
       "3696241                  35.0                  0.0  57.0       11.0   \n",
       "3696242                   6.0                  0.0  57.0       18.0   \n",
       "3696243                  19.0                  0.0  68.0       -2.0   \n",
       "3696244                   1.0                  0.0  62.0       17.0   \n",
       "\n",
       "         Asparate Aminotransferase (AST)  Bilirubin, Total  Calcium, Total  \\\n",
       "0                                   -2.0              -2.0             9.0   \n",
       "1                                  341.0               0.5             9.5   \n",
       "2                                   11.0               0.3             9.7   \n",
       "3                                   72.0               0.4             8.9   \n",
       "4                                   -2.0              -2.0            -2.0   \n",
       "...                                  ...               ...             ...   \n",
       "3696240                             -2.0               8.3            -2.0   \n",
       "3696241                             53.0               2.0             8.0   \n",
       "3696242                             15.0               1.1             9.9   \n",
       "3696243                             -2.0              -2.0            -2.0   \n",
       "3696244                             -2.0              -2.0             8.9   \n",
       "\n",
       "         Calculated Total CO2  Chloride  Creatinine  ...    MCV  Magnesium  \\\n",
       "0                        26.0     105.0         1.1  ...   92.0        1.9   \n",
       "1                        -2.0     105.0         1.3  ...  101.0        1.9   \n",
       "2                        -2.0      98.0         3.1  ...   99.0        1.6   \n",
       "3                        23.0     118.0         0.9  ...   91.0        2.1   \n",
       "4                        -2.0      -2.0        -2.0  ...   -2.0       -2.0   \n",
       "...                       ...       ...         ...  ...    ...        ...   \n",
       "3696240                  -2.0      -2.0        -2.0  ...   -2.0       -2.0   \n",
       "3696241                  21.0      99.0         0.6  ...   94.0        1.5   \n",
       "3696242                  29.0      86.0         1.4  ...   85.0        1.9   \n",
       "3696243                  -2.0      -2.0        -2.0  ...   -2.0       -2.0   \n",
       "3696244                  31.0     100.0         3.5  ...   99.0        1.8   \n",
       "\n",
       "           PT  Phosphate  Sodium  Urea Nitrogen  White Blood Cells  \\\n",
       "0        12.1        3.3   140.0           17.0               10.9   \n",
       "1        13.2        3.3   136.0           21.0                6.5   \n",
       "2        12.2        3.8   138.0           37.0                8.6   \n",
       "3        11.8        3.2   134.0           21.0                8.3   \n",
       "4        -2.0       -2.0    -2.0           -2.0               -2.0   \n",
       "...       ...        ...     ...            ...                ...   \n",
       "3696240  -2.0       -2.0    -2.0           -2.0               -2.0   \n",
       "3696241  16.0        2.4   131.0           10.0                6.9   \n",
       "3696242   9.7        3.4   128.0           37.0                8.2   \n",
       "3696243  -2.0       -2.0    -2.0           -2.0               -2.0   \n",
       "3696244  -2.0        3.0   142.0           13.0                4.6   \n",
       "\n",
       "         Ethnicity_NUM  Gender_NUM  TARGET_NUM  \n",
       "0                  0.0         0.0         0.0  \n",
       "1                  0.0         0.0         1.0  \n",
       "2                  0.0         0.0         1.0  \n",
       "3                  0.0         0.0         2.0  \n",
       "4                  0.0         1.0         3.0  \n",
       "...                ...         ...         ...  \n",
       "3696240            0.0         0.0        18.0  \n",
       "3696241            0.0         1.0         7.0  \n",
       "3696242            6.0         0.0         8.0  \n",
       "3696243            1.0         1.0         4.0  \n",
       "3696244            0.0         0.0         5.0  \n",
       "\n",
       "[3696245 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c630df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_filt['MMM'] = XY_train_filt[['Hematocrit', 'Hemoglobin', 'MCV', 'MCH']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fb6ecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition_importance</th>\n",
       "      <th>Hospital_death_flag</th>\n",
       "      <th>Age</th>\n",
       "      <th>Anion Gap</th>\n",
       "      <th>Asparate Aminotransferase (AST)</th>\n",
       "      <th>Bilirubin, Total</th>\n",
       "      <th>Calcium, Total</th>\n",
       "      <th>Calculated Total CO2</th>\n",
       "      <th>Chloride</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>...</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>PT</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Urea Nitrogen</th>\n",
       "      <th>White Blood Cells</th>\n",
       "      <th>Ethnicity_NUM</th>\n",
       "      <th>Gender_NUM</th>\n",
       "      <th>TARGET_NUM</th>\n",
       "      <th>MMM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>12.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>140.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>136.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>138.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>134.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696240</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696241</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>131.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696242</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>128.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696243</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696244</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>31.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3696245 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Condition_importance  Hospital_death_flag   Age  Anion Gap  \\\n",
       "0                         4.0                  0.0  56.0       13.0   \n",
       "1                         7.0                  0.0  46.0       14.0   \n",
       "2                         1.0                  0.0  69.0       18.0   \n",
       "3                        10.0                  0.0  76.0       12.0   \n",
       "4                         8.0                  0.0  91.0       -2.0   \n",
       "...                       ...                  ...   ...        ...   \n",
       "3696240                   3.0                  0.0   0.0       -2.0   \n",
       "3696241                  35.0                  0.0  57.0       11.0   \n",
       "3696242                   6.0                  0.0  57.0       18.0   \n",
       "3696243                  19.0                  0.0  68.0       -2.0   \n",
       "3696244                   1.0                  0.0  62.0       17.0   \n",
       "\n",
       "         Asparate Aminotransferase (AST)  Bilirubin, Total  Calcium, Total  \\\n",
       "0                                   -2.0              -2.0             9.0   \n",
       "1                                  341.0               0.5             9.5   \n",
       "2                                   11.0               0.3             9.7   \n",
       "3                                   72.0               0.4             8.9   \n",
       "4                                   -2.0              -2.0            -2.0   \n",
       "...                                  ...               ...             ...   \n",
       "3696240                             -2.0               8.3            -2.0   \n",
       "3696241                             53.0               2.0             8.0   \n",
       "3696242                             15.0               1.1             9.9   \n",
       "3696243                             -2.0              -2.0            -2.0   \n",
       "3696244                             -2.0              -2.0             8.9   \n",
       "\n",
       "         Calculated Total CO2  Chloride  Creatinine  ...  Magnesium    PT  \\\n",
       "0                        26.0     105.0         1.1  ...        1.9  12.1   \n",
       "1                        -2.0     105.0         1.3  ...        1.9  13.2   \n",
       "2                        -2.0      98.0         3.1  ...        1.6  12.2   \n",
       "3                        23.0     118.0         0.9  ...        2.1  11.8   \n",
       "4                        -2.0      -2.0        -2.0  ...       -2.0  -2.0   \n",
       "...                       ...       ...         ...  ...        ...   ...   \n",
       "3696240                  -2.0      -2.0        -2.0  ...       -2.0  -2.0   \n",
       "3696241                  21.0      99.0         0.6  ...        1.5  16.0   \n",
       "3696242                  29.0      86.0         1.4  ...        1.9   9.7   \n",
       "3696243                  -2.0      -2.0        -2.0  ...       -2.0  -2.0   \n",
       "3696244                  31.0     100.0         3.5  ...        1.8  -2.0   \n",
       "\n",
       "         Phosphate  Sodium  Urea Nitrogen  White Blood Cells  Ethnicity_NUM  \\\n",
       "0              3.3   140.0           17.0               10.9            0.0   \n",
       "1              3.3   136.0           21.0                6.5            0.0   \n",
       "2              3.8   138.0           37.0                8.6            0.0   \n",
       "3              3.2   134.0           21.0                8.3            0.0   \n",
       "4             -2.0    -2.0           -2.0               -2.0            0.0   \n",
       "...            ...     ...            ...                ...            ...   \n",
       "3696240       -2.0    -2.0           -2.0               -2.0            0.0   \n",
       "3696241        2.4   131.0           10.0                6.9            0.0   \n",
       "3696242        3.4   128.0           37.0                8.2            6.0   \n",
       "3696243       -2.0    -2.0           -2.0               -2.0            1.0   \n",
       "3696244        3.0   142.0           13.0                4.6            0.0   \n",
       "\n",
       "         Gender_NUM  TARGET_NUM     MMM  \n",
       "0               0.0         0.0  44.350  \n",
       "1               0.0         1.0  46.275  \n",
       "2               0.0         1.0  41.200  \n",
       "3               0.0         2.0  44.025  \n",
       "4               1.0         3.0  -2.000  \n",
       "...             ...         ...     ...  \n",
       "3696240         0.0        18.0  -2.000  \n",
       "3696241         1.0         7.0  43.575  \n",
       "3696242         0.0         8.0  42.625  \n",
       "3696243         1.0         4.0  -2.000  \n",
       "3696244         0.0         5.0  42.200  \n",
       "\n",
       "[3696245 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b47589e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_filt['ACS'] = XY_train_filt[['Anion Gap', 'Chloride', 'Sodium']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6168a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_filt['ACS'] = XY_train_filt[['Calcium, Total', 'Magnesium', 'Phosphate']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e3bd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_drop = ['Hematocrit', 'Hemoglobin', 'MCV', 'MCH'] +\\\n",
    "                   ['Anion Gap', 'Chloride', 'Sodium'] + ['Calcium, Total', 'Magnesium', 'Phosphate']\n",
    "XY_train_filt = XY_train_filt.drop(columns_for_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd0907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition_importance</th>\n",
       "      <th>Hospital_death_flag</th>\n",
       "      <th>Age</th>\n",
       "      <th>Asparate Aminotransferase (AST)</th>\n",
       "      <th>Bilirubin, Total</th>\n",
       "      <th>Calculated Total CO2</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>I</th>\n",
       "      <th>L</th>\n",
       "      <th>PT</th>\n",
       "      <th>Urea Nitrogen</th>\n",
       "      <th>White Blood Cells</th>\n",
       "      <th>Ethnicity_NUM</th>\n",
       "      <th>Gender_NUM</th>\n",
       "      <th>TARGET_NUM</th>\n",
       "      <th>MMM</th>\n",
       "      <th>ACS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.350</td>\n",
       "      <td>4.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.275</td>\n",
       "      <td>4.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.200</td>\n",
       "      <td>5.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.025</td>\n",
       "      <td>4.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696240</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696241</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.575</td>\n",
       "      <td>3.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696242</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.625</td>\n",
       "      <td>5.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696243</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696244</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.200</td>\n",
       "      <td>4.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3696245 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Condition_importance  Hospital_death_flag   Age  \\\n",
       "0                         4.0                  0.0  56.0   \n",
       "1                         7.0                  0.0  46.0   \n",
       "2                         1.0                  0.0  69.0   \n",
       "3                        10.0                  0.0  76.0   \n",
       "4                         8.0                  0.0  91.0   \n",
       "...                       ...                  ...   ...   \n",
       "3696240                   3.0                  0.0   0.0   \n",
       "3696241                  35.0                  0.0  57.0   \n",
       "3696242                   6.0                  0.0  57.0   \n",
       "3696243                  19.0                  0.0  68.0   \n",
       "3696244                   1.0                  0.0  62.0   \n",
       "\n",
       "         Asparate Aminotransferase (AST)  Bilirubin, Total  \\\n",
       "0                                   -2.0              -2.0   \n",
       "1                                  341.0               0.5   \n",
       "2                                   11.0               0.3   \n",
       "3                                   72.0               0.4   \n",
       "4                                   -2.0              -2.0   \n",
       "...                                  ...               ...   \n",
       "3696240                             -2.0               8.3   \n",
       "3696241                             53.0               2.0   \n",
       "3696242                             15.0               1.1   \n",
       "3696243                             -2.0              -2.0   \n",
       "3696244                             -2.0              -2.0   \n",
       "\n",
       "         Calculated Total CO2  Creatinine    I    L    PT  Urea Nitrogen  \\\n",
       "0                        26.0         1.1 -2.0 -2.0  12.1           17.0   \n",
       "1                        -2.0         1.3 -2.0 -2.0  13.2           21.0   \n",
       "2                        -2.0         3.1  1.0  4.0  12.2           37.0   \n",
       "3                        23.0         0.9  1.0  4.0  11.8           21.0   \n",
       "4                        -2.0        -2.0 -2.0 -2.0  -2.0           -2.0   \n",
       "...                       ...         ...  ...  ...   ...            ...   \n",
       "3696240                  -2.0        -2.0 -2.0 -2.0  -2.0           -2.0   \n",
       "3696241                  21.0         0.6 -2.0 -2.0  16.0           10.0   \n",
       "3696242                  29.0         1.4 -2.0 -2.0   9.7           37.0   \n",
       "3696243                  -2.0        -2.0 -2.0 -2.0  -2.0           -2.0   \n",
       "3696244                  31.0         3.5 -2.0 -2.0  -2.0           13.0   \n",
       "\n",
       "         White Blood Cells  Ethnicity_NUM  Gender_NUM  TARGET_NUM     MMM  \\\n",
       "0                     10.9            0.0         0.0         0.0  44.350   \n",
       "1                      6.5            0.0         0.0         1.0  46.275   \n",
       "2                      8.6            0.0         0.0         1.0  41.200   \n",
       "3                      8.3            0.0         0.0         2.0  44.025   \n",
       "4                     -2.0            0.0         1.0         3.0  -2.000   \n",
       "...                    ...            ...         ...         ...     ...   \n",
       "3696240               -2.0            0.0         0.0        18.0  -2.000   \n",
       "3696241                6.9            0.0         1.0         7.0  43.575   \n",
       "3696242                8.2            6.0         0.0         8.0  42.625   \n",
       "3696243               -2.0            1.0         1.0         4.0  -2.000   \n",
       "3696244                4.6            0.0         0.0         5.0  42.200   \n",
       "\n",
       "              ACS  \n",
       "0        4.733333  \n",
       "1        4.900000  \n",
       "2        5.033333  \n",
       "3        4.733333  \n",
       "4       -2.000000  \n",
       "...           ...  \n",
       "3696240 -2.000000  \n",
       "3696241  3.966667  \n",
       "3696242  5.066667  \n",
       "3696243 -2.000000  \n",
       "3696244  4.566667  \n",
       "\n",
       "[3696245 rows x 17 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e010b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cn, max_value in XY_train_filt.max().items():\n",
    "    if cn != \"TARGET_NUM\":\n",
    "        XY_train_filt[cn] = XY_train_filt[cn] / max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "264128b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = list(XY_train_filt.columns)\n",
    "train_columns.remove(\"TARGET_NUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91d7a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = XY_train_filt[train_columns].values\n",
    "y_train = to_categorical(XY_train_filt[\"TARGET_NUM\"].values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34cc5320",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "hiddel_layer (Dense)         (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "hiddel_layer_2 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 39,699\n",
      "Trainable params: 39,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "24765/24765 [==============================] - 98s 4ms/step - loss: 2.3105 - accuracy: 0.2668 - val_loss: 2.2846 - val_accuracy: 0.2719\n",
      "Epoch 2/15\n",
      "24765/24765 [==============================] - 103s 4ms/step - loss: 2.2815 - accuracy: 0.2736 - val_loss: 2.2755 - val_accuracy: 0.2746\n",
      "Epoch 3/15\n",
      "24765/24765 [==============================] - 96s 4ms/step - loss: 2.2745 - accuracy: 0.2752 - val_loss: 2.2734 - val_accuracy: 0.2753\n",
      "Epoch 4/15\n",
      "24765/24765 [==============================] - 98s 4ms/step - loss: 2.2703 - accuracy: 0.2763 - val_loss: 2.2706 - val_accuracy: 0.2757\n",
      "Epoch 5/15\n",
      "24765/24765 [==============================] - 97s 4ms/step - loss: 2.2676 - accuracy: 0.2772 - val_loss: 2.2676 - val_accuracy: 0.2770\n",
      "Epoch 6/15\n",
      "24765/24765 [==============================] - 97s 4ms/step - loss: 2.2653 - accuracy: 0.2777 - val_loss: 2.2664 - val_accuracy: 0.2764\n",
      "Epoch 7/15\n",
      "24765/24765 [==============================] - 98s 4ms/step - loss: 2.2636 - accuracy: 0.2781 - val_loss: 2.2644 - val_accuracy: 0.2780\n",
      "Epoch 8/15\n",
      "24765/24765 [==============================] - 103s 4ms/step - loss: 2.2622 - accuracy: 0.2783 - val_loss: 2.2626 - val_accuracy: 0.2778\n",
      "Epoch 9/15\n",
      "24765/24765 [==============================] - 100s 4ms/step - loss: 2.2611 - accuracy: 0.2786 - val_loss: 2.2631 - val_accuracy: 0.2779\n",
      "Epoch 10/15\n",
      "24765/24765 [==============================] - 99s 4ms/step - loss: 2.2601 - accuracy: 0.2789 - val_loss: 2.2636 - val_accuracy: 0.2778\n",
      "Epoch 11/15\n",
      "24765/24765 [==============================] - 99s 4ms/step - loss: 2.2592 - accuracy: 0.2791 - val_loss: 2.2608 - val_accuracy: 0.2784\n",
      "Epoch 12/15\n",
      "24765/24765 [==============================] - 98s 4ms/step - loss: 2.2585 - accuracy: 0.2794 - val_loss: 2.2600 - val_accuracy: 0.2788\n",
      "Epoch 13/15\n",
      "24765/24765 [==============================] - 103s 4ms/step - loss: 2.2578 - accuracy: 0.2795 - val_loss: 2.2608 - val_accuracy: 0.2783\n",
      "Epoch 14/15\n",
      "24765/24765 [==============================] - 104s 4ms/step - loss: 2.2572 - accuracy: 0.2798 - val_loss: 2.2598 - val_accuracy: 0.2785\n",
      "Epoch 15/15\n",
      "24765/24765 [==============================] - 98s 4ms/step - loss: 2.2567 - accuracy: 0.2798 - val_loss: 2.2600 - val_accuracy: 0.2786\n"
     ]
    }
   ],
   "source": [
    "model = define_model(inpit_shape=16)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d559b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup_nb",
   "language": "python",
   "name": "jup_nb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
